&emsp;
# 相机模型

这个模型有很多种，其中最简单的称为`针孔模型`。针孔模型是很常用，而且有效的模型，它描述了一束光线通过针孔之后，在针孔背面投影成像的关系


&emsp;
## 1 蜡烛投影
在一个暗箱的前方放着一支点燃的蜡烛，蜡烛的光透过暗箱上的一个小孔投影在暗箱的后方平面上，并在这个平面上形成了一个倒立的蜡烛图像。在这个过程中，小孔模型能够把三维世界中的蜡烛投影到一个二维成像平面

<div align="center">
    <image src="./imgs/cameraModel.png" width=500 >
</div>


&emsp;
## 2 针孔相机模型建模
现在来对这个简单的针孔模型进行几何建模。设：
- $O_{xyz}$ 为相机坐标系
  - $O$ 为摄像机的光心，也是针孔模型中的针孔
  - [right, down, forward]，让 $z$ 轴指向相机前方，$x$ 向右，$y$ 向下（不同的软件有自己的定义）
- 现实世界的空间点 $P$，经过小孔 $O$ 投影之后，落在物理成像平面 $O'_{x'y'z'}$ 上，像点为 $P'$
  - $P$ 的坐标: $[X, Y, Z]^T$
  - $P'$的坐标: $[X', Y', Z']^T$
- 并且设物理成像平面到小孔的距离为 $f$（焦距）


那么，根据三角形相似关系，有：
$$\frac{Z}{f} = -\frac{X}{X'} = -\frac{Y}{Y'} $$

其中负号表示成的像是倒立的。为了简化模型，我们把可以成像平面对称到相机前方，和三维空间点一起放在摄像机坐标系的同一侧。这样做可以把公式中的负号去掉，使式子更加简洁：

$$\frac{Z}{f} = \frac{X}{X'} = \frac{Y}{Y'} $$

<div align="center">
    <image src="./imgs/cameraModel.png" width=500 >
</div>
&emsp;

整理得：
$$\begin{cases}X' = f\frac{X}{Z}\\ 
\\ Y' = f\frac{Y}{Z} \end{cases}$$

>project1
```c++
cv::Point2f project1(const cv::Point3f &p3D, float focal_length=500)
{
    return cv::Point2f(
        p3D.x*focal_length / p3D.z,
        p3D.y*focal_length / p3D.z);
}
```


&emsp;
## 3 从物体到像素
在相机中，我们最终获得的是图片上一个个的像素，这需要在成像平面上对像进行采样和量化

假设设在物理成像平面上固定着一个像素平面 `o-u-v`, 这个平面大小不一定, 可能是 800x600、1600x900。我们在像素平面得到了 $P'$ 的像素坐标：$[u, v]^T$

像素坐标系通常的定义方式是：原点 $o'$ 位于图像的左上角，$u$ 轴向右与 $x$ 轴平行，$v$ 轴向下与 $y$ 轴平行。像素坐标系与成像平面之间，相差了两个量:
- 一个缩放, 设像素坐标
    - 在 $u$ 轴上缩放了 $α$ 倍
    - 在 $v$ 轴上缩放了 $β$ 倍
- 一个原点的平移：设原点平移了 $[c_x, c_y]^T$

那么，$P'$ 的坐标与像素坐标 $[u, v]^T$ 的关系为：

$$\begin{cases} u = \alpha X' + c_x \\
v = \beta Y' + c_y\end{cases}$$

>project2
```c++
cv::Point2f project2(const cv::Point3f &p3D, 
    float alpha=1, float beta=1, float focal_length=500, 
    float cx=320, float cy=240)
{
    return cv::Point2f(
        alpha*(focal_length*p3D.x/p3D.z) + cx,
        beta *(focal_length*p3D.y/p3D.z) + cy);
}
```

将这个式子：
$$\begin{cases}X' = f\frac{X}{Z}\\ 
\\ Y' = f\frac{Y}{Z} \end{cases}$$

代入，并把 $\alpha f$ 合并成 $f_x$，把 $\beta f$ 合并成 $f_y$，得：

$$\begin{cases} u = f_x \frac{X}{Z} + c_x \\
v = f_y \frac{Y}{Z}  + c_y\end{cases}$$

- $f$ 的单位：m（米）
- $α， β$ 的单位：pix/m（像素每米）
- $fx, fy$ 的单位：pix（像素）

>project3
```c++
std::vector<float> parameters = {500, 500, 320, 240}; // fx, fy, cx, cy

cv::Point2f project3(const cv::Point3f &p3D, const std::vector<float> &parameters)
{
    return cv::Point2f(
        parameters[0]*p3D.x + parameters[2],
        parameters[1]*p3D.x + parameters[3]);
}
```

&emsp;
## 4 坐标转换

>相机坐标 -> 像素坐标
$$\begin{cases} u = f_x \frac{X}{Z} + c_x \\
v = f_y \frac{Y}{Z}  + c_y\end{cases}$$
- $X，Y，Z$：相机坐标的 $X，Y，Z$（深度 depth）
- $f_x，f_y$：缩放系数和焦距的乘积
- $c_x，c_y$：中心偏移量
- $u，v$：像素坐标
  ```py
  import numpy as np

  def point2pixel(P, focal_length=500, alpha=1, beta=1, cx=320, cy=240):
      X, Y, Z = P
      fx = alpha * focal_length
      fy = beta  * focal_length
      u = fx * (X/Z) + cx
      v = fy * (Y/Z) + cy
      return u, v

  if __name__ == '__main__':
      P = np.array([2, 4, 6])

      u, v = point2pixel(P)
      print(f"P project to (u, v) = ({u}, {v})")
  ```

&emsp;
>像素坐标 -> 相机坐标
- 需要知道深度 $Z$（depth），单目相机获取困难且误差大，双目和深度相机可以获取
- 这个过程跟我们实际情况很像，因为通常使用 RGB-D 相机会得到两张图：
  - image: RGB图，包含 u, v 和 RGB 信息
  - depth: 深度图，包含 Z 的信息

  $$\begin{cases} X = (u - c_x)/f_x \times Z  \\
  Y = (v - c_y)/f_y \times Z \end{cases}$$
  ```py
  import numpy as np

  def pixel2camera(u, v, depth, fx=500, fy=500, cx=320, cy=240):
      X = (u - cx) / fx * depth
      Y = (v - cy) / fy * depth
      return X, Y, depth
      

  if __name__ == '__main__':
      Pu, Pv = (128, 88)
      Pdepth = 620
      X, Y, Z = pixel2camera(Pu, Pv, Pdepth)
      print(f"Transform pixel (u, v) to ({X}, {Y}, {Z})")
  ```